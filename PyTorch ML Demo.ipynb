{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3543329-73ca-4c4d-9f4a-18edd6c0d72e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## General Overview\n",
    "\n",
    "Neural Networks approximate multidimensional functions like $W_1x_1+W_2x_2+W_3x_3 + b = p$.  The neural network learns the values of $W_1, W_2, W_3, b$.  Where $W_i$ are the weights of the equation, $b$ is the bias term of the equation, and $x_i$ is the input to the equation.\n",
    "\n",
    "To illustrate the training process lets walk through a simplified toy example:  we want the neural network to predict if the client will default on their credit card payments.  If the client defaults the network will output 1, if the client doesn't then it will output 0.  Let the client under question be a client that default i.e. is labelled 1.  If the neural net is working properly then $W_1x_1+W_2x_2+W_3x_3= 1$. Let the client be defined by 3 features (or variables) $x_1, x_2, x_3$. These features will be the last 3 statement payments: $x_1=300, x_2=200, x_3=100$.  The network initially guesses that $W_1=.0009, W_2=.0008, W_3=.0007$. This means the answer to $W_1x_1+W_2x_2+W_3x_3 = .0009 \\cdot 3-- + .0008 \\cdot 200 + .0007 \\cdot 100 = .46$. How close was that guess? Use a linear loss function by taking the difference between label of the client (1) and the networks guess (.46) thus 1-.46 = .54. This number (.54) is the error. You can use calculus to determine how much each weight contributed to the error. This is often called the gradient or the error signal.  You then adjust the weight by its contribution to minimize the error. You repeat this process hundreds or thousands of times for every client in the dataset. Eventually, the neural network learns to identify which clients default and which don't.\n",
    "\n",
    "Ultimately, we are trying to minimize our error i.e. our loss function.  In the above example, we used a linear loss function to calculate the error.  However, linear loss functions are not very effective because they do not penalize large errors well nor do they provide good error signals.  As a result, other loss functions are used such as the Mean Squared Error: $\\frac{1}{N}\\sum\\limits_{n=0}^{N}(y-\\hat{y})^2$ where $y$ is the label and \\hat{y}$ is the network output.  This loss function is quadratic and forms a curve:\n",
    "\n",
    "\n",
    "<img\n",
    "  src ='https://i.ibb.co/zRV9pXj/MSE-plot.png'\n",
    "  alt=\"MSE_plot.png\"\n",
    "  width=\"300\" \n",
    "  height=\"256\"\n",
    "/>\n",
    "\n",
    "The minimum of the MSE loss function is 0. The gradients are tangent functions and by taking the negative of the gradients they lead us to the minimum of the loss function.  Thus each gradient is a step towards the minimum:\n",
    "\n",
    "<img\n",
    "  src ='https://i.ibb.co/LrsW0HS/gradient-desc.png'\n",
    "  alt=\"gradient_desc.png\"\n",
    "  width=\"300\" \n",
    "  height=\"256\"\n",
    "/>\n",
    "\n",
    "Different loss functions may have different curves, for example:\n",
    "\n",
    "<img\n",
    "  src ='https://i.ibb.co/gZDbmv5/local-global.png'\n",
    "  alt=\"local_global.png\"\n",
    "  width=\"300\" \n",
    "  height=\"256\"\n",
    "/>\n",
    "\n",
    "Ideally, we find the global minimum but we could become trapped in local minimums instead.  Later in this notebook, we explore strategies for avoiding local minimum.\n",
    "\n",
    "\\* To simplify the example, we drop the bias term, $b$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "711d03a9-ff7c-4528-8874-547239efa0ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "437e20bf-7bf5-42df-b6ae-32cc39cf05a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import libraries for data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import libraries for training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set device variable according to availability of gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3694ad02-6960-4f6d-9d93-726b0ff156b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "This process consists of splitting the data into features (X) and labels (y).  The Neural Network learns to predict the labels from the features.\n",
    "Additionally, the data is split into train and test sets.  Test sets are used to evaluate the model.  The model should never see the test set until it is being evaluated.\n",
    "\n",
    "The data is normalized to ensure that all features contribute equally to the learning process.   This ensures that the model doesn't disproportionately favor certain features based on their scale or range.  Additionally, normalizing removes large outliers in the data.  Large outliers can distort the gradients during optimization, leading to instability or slow convergence during training. If features have extreme values, they can cause large weight updates that make it harder for the model to find a stable solution. By normalizing or removing outliers, the gradients during backpropagation become more stable, allowing the optimization algorithm (e.g., gradient descent) to converge more efficiently and effectively.\n",
    "\n",
    "Finally, dataLoader objects are created to handle batch processing during training and testing.  Batch processing divides the training data into smaller subsets (i.e. batches) instead of feeding the entire dataset to the model at once. These smaller batches are processed in iterations, where the model updates its parameters after each batch.  This iterative approach helps speed up training by allowing the model to make incremental adjustments.  Batch processing also introduces some noise into the training process because each batch represents only a small, random subset of the data.  The small, random subsets highlight different patterns or correlations in the data allowing the model to take different steps during gradient descent.  This variability allows the model to escape local minima and explore a broader region of the parameter space.  This helps prevents overfitting and creates a model with better generalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a66a017-a376-4943-ae90-2d246bf7d5a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read csv file with PySpark\n",
    "df_spark = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"sep\", \",\").load(\"/FileStore/tables/default_of_credit_card_clients-5.csv\")\n",
    "\n",
    "# transform from PySpark dataframe -> pandas dataframe -> numpy array (see documentation: pyspark.pandas.DataFrame.to_numpy)\n",
    "data = df_spark.toPandas().to_numpy()\n",
    "\n",
    "# create train and test sets\n",
    "test_size = 0.2\n",
    "seed = 0\n",
    "X, y = data[:, 1:-1], data[:, -1] #Drop ID col, last col is y, rest is X.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "# normalize data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# convert data to PyTorch tensors (TODO: find method to go from PySpark dataframe to Torch tensor directly)\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5b7b10-a652-4044-a812-82ab133c5444",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define the Model\n",
    "\n",
    "In this working example, the Multi-Layer Perceptrons (MLP) will be a flexible model, constructed dynamically, that can handle different numbers of hidden layers and hidden neurons. The model will also use either ReLU or Tanh as the activation function between layers.\n",
    "\n",
    "### Model Layers\n",
    "The MLP consist of 3 layers: input, hidden, and output with activation functions in between each layer.\n",
    "\n",
    "<img\n",
    "  src ='https://i.ibb.co/DMJKNR0/mlp-diag.png'\n",
    "  alt=\"local_global.png\"\n",
    "  width=\"484\" \n",
    "  height=\"206\"\n",
    "/>\n",
    "\n",
    "#### Input Layer\n",
    "The input layer recieves the data.  Its size (i.e. number of neurons) corresponds to the number of features in the data.  This determines how much information is provided to the network.  The network cannot process more than it is given so each input feature should represent some meaningful aspect of the data for the model to learn effectively.  For this working example, the data has 24 features therefore the input layer will have 24 neurons.\n",
    "\n",
    "#### Hidden Layer\n",
    "The hidden layers sit between the input and output layers.  The weights and biases of the hidden layers capture and model the complex patterns and interactions in the data.  The number of hidden layers and the number of neurons in each layer can vary.  However, if the hidden layers are too small (too few neurons), the model might not have enough capacity to capture complex patterns. On the other hand, if the hidden layers are too large (too many neurons), the network may overfit, meaning it will learn to model the training data very well but might not generalize to new, unseen data.  The number of layers describes how \"deep\" the model is.  Today, \"deep learning\" models consists of 1000+ layers, however, in theory, it is any model with more than 3 layers.\n",
    "\n",
    "In this working example, we experiment with different hidden layer sizes allowing us to explore different model capacities and find the best trade-off between underfitting and overfitting:\n",
    "\n",
    "**Baseline** model is a linear mapping between the input and the output with no hidden layers.  If it performs poorly it demonstrates a need for additional non-linearity.\n",
    "\n",
    "**Average of the input features and output labels** presents a balanced paradigm, where the hidden layer size is large enough to capture some complexity but not too large to risk overfitting. It's often a reasonable first guess when exploring MLP architectures because it creates a middle ground between a small and large hidden layer, providing some capacity to model non-linearities without making the model too complex.\n",
    "\n",
    "**Hidden layer < input layer** forced the model to generalize better by limiting its capacity. If the problem doesn't require a large number of neurons to learn patterns effectively, reducing the hidden layer size can prevent overfitting and lead to faster training and better generalization to new data. This size also reflects the assumption that the output is simpler than the input data, as it represents a slightly reduced complexity.\n",
    "\n",
    "**Hidden layer > input layer** increases the model’s capacity to learn more complex patterns. This can be helpful if the data is highly non-linear or has intricate relationships between the features and the target labels. A larger hidden layer gives the model more flexibility, but it also increases the risk of overfitting, especially if the dataset is small or noisy. This experiment tests whether the model benefits from increased complexity and capacity, or if it struggles with overfitting.\n",
    "\n",
    "#### Output Layer\n",
    "The output layer is the final layer of the neural networ that produces the predictions.  The number of neurons in the output layer corresponds to the number of possible outputs or classes.  This working example is a binary classification problem therefore the output layer will have 2 neurons.\n",
    "\n",
    "### Activation Functions\n",
    "\n",
    "The activation functions introduce non-linearity to the network.  Each layer represents lineary matrix arithmetic Ax+b=y where A is the layer weights, x is the input, b the layer bias and y the output.  If there are two layers, layer 1: $A_1x_1 + b_1 = y_1$ and layer 2: \\\\(A_2 x_2 + b_2 = y_2 \\\\).  If the output of layer 1 is the input to layer 2 then layer 2 becomes \\\\(A_2 x_2 + b_2 = A_2 (A_1 x_1 + b_1) + b_2 = y_2 \\\\) which collapses back into the linear equation \\\\(Ax+b\\\\).  However, wrapping layer outputs in a non-linear function such as a sigmoid prevents this.  In this working example, we will experiment with two common activation functions, Rectified Linear Unit (ReLU) and Tanh. \n",
    "\n",
    "**ReLU** is a computationally efficient activation function because it is a simple thresholding operation with the beneficial property of producing sparse activations that reduces the computational load of the model (i.e. only a portion of the neurons are activated (non-zero) at any given time).  However, the sparsity can also be a drawback because neuron's can \"die\" i.e. get stuck output zero for all inputs if the weights are not properly adjusted.  Additionally, the ReLU function is unbounded which means that large values can pass through leading to numerical instability.\n",
    "\n",
    "**Tanh** has a bounded output between -1 and 1 which helps mitigate extreme values.  This makes it suitable for deep learning however this bounded output can lead to a vanishing gradient for extreme values which slow covergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ed65ac-3203-4f53-9c10-52f5d0e41b59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# this will be saved in the hyperparameter grid later\n",
    "x_features = X_train.shape[1]\n",
    "y_labels = y_train.shape[0]\n",
    "\n",
    "hidden_layer_list = [\n",
    "        [],\n",
    "        [int((x_features + y_labels) / 2)],\n",
    "        [int(x_features * 2 / 3 + y_labels)],\n",
    "        [int(x_features * 3 / 2)],\n",
    "    ]\n",
    "\n",
    "# Define the MLP Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_sizes, output_size, activation='relu'):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # Input Layer\n",
    "        prev_size = input_size\n",
    "\n",
    "        # Hidden Layer(s)\n",
    "        for hidden_size in hidden_layer_sizes:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            if activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            prev_size = hidden_size\n",
    "\n",
    "        # Output Layer\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b4536b-3d54-4f0d-8db2-8fb1a2d9151e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define Train and Evaluate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21379caa-2304-472a-a302-c61191d4e2f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, criterion, train_loader, num_epochs=100):\n",
    "    model.train()\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        #running_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        #  running_loss += loss.item() - used to chart loss during training\n",
    "\n",
    "def evaluate_model(model, criterion, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    outputs_total = []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            total_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            outputs_total.extend(preds.flatten().tolist())\n",
    "    return total_loss / len(data_loader), correct / len(data_loader.dataset), outputs_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "759f28dd-1d3d-42b1-b121-f3397c3bc7f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Define Hyperparameters\n",
    "\n",
    "### Learning Rate\n",
    "Scales the modification made to the models weight by the gradient.  In other words, the leraning rate defines how large a step is made in the direction that minimizes the error.  Smaller learning rates allow for finer adjustments and more precise convergence.  However, it can lead slower training with the risk of getting stuck in suboptimal local minima.  Large training rates lead to faster covergence but they can overshoot the optimal solution causing the model to oscilate around the minimum failing to converge and leading to unstable training.  Generally, adaptive learning rates are used where larger learning rates are used at the beginning of the training and smaller learning rates at the end.\n",
    "\n",
    "### Regularization\n",
    "Prevent overfitting by adding a penalty to the loss function based on the size of the weights, encouraging the model to keep the weights small. This helps to simplify the model and reduce its complexity, making it more likely to generalize well to new, unseen data.\n",
    "\n",
    "### Momentum\n",
    "Accumulates past gradient information to build up speed in the direction that reduces the loss function, while dampening oscillations in directions that are less important. This can lead to faster convergence, particularly in scenarios where the loss function has narrow, steep valleys and wide flat regions, or when there are noisy gradients.  A common value used is .9 because it allows more of the previous gradients to be retained. This helps the optimizer push through shallow areas of the loss function and speed up convergence.\n",
    "\n",
    "### Epoch\n",
    "Refers to one complete pass through the entire training dataset during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4affb3c-161f-4007-8d7a-48fb329016bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameter grid\n",
    "\n",
    "# Use this if you want to experiment with different hyper parameter configurations\n",
    "#param_grid = {\n",
    "#    \"learning_rate_init\": np.logspace(-3, -1, 3),\n",
    "#    \"alpha\": np.logspace(-4, -2, 3),\n",
    "#    \"activation\": ['relu', 'tanh'],\n",
    "#    \"hidden_layer_sizes\": hidden_layer_list\n",
    "#}\n",
    "\n",
    "# param grid to train a single model\n",
    "param_grid = {\n",
    "    \"learning_rate_init\": np.array(0.001),\n",
    "    \"alpha\": np.array(0),\n",
    "    \"activation\": ['tanh'],\n",
    "    \"hidden_layer_sizes\": [int((x_features + y_labels) / 2)]\n",
    "}\n",
    "\n",
    "momentum = 0.9\n",
    "num_epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec5b7e67-fed9-4e09-a70c-c76ffe039a4d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a8db30c-8e62-4f42-90c5-4d6ffab2f46a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\rEpochs:   0%|          | 0/500 [00:00<?, ?it/s]\rEpochs:   0%|          | 1/500 [00:02<20:56,  2.52s/it]\rEpochs:   0%|          | 2/500 [00:04<19:17,  2.32s/it]\rEpochs:   1%|          | 3/500 [00:06<18:38,  2.25s/it]\rEpochs:   1%|          | 4/500 [00:09<18:21,  2.22s/it]\rEpochs:   1%|          | 5/500 [00:11<18:27,  2.24s/it]\rEpochs:   1%|          | 6/500 [00:13<19:31,  2.37s/it]\rEpochs:   1%|▏         | 7/500 [00:16<20:20,  2.48s/it]\rEpochs:   2%|▏         | 8/500 [00:19<20:29,  2.50s/it]\rEpochs:   2%|▏         | 9/500 [00:21<20:00,  2.44s/it]\rEpochs:   2%|▏         | 10/500 [00:23<19:26,  2.38s/it]\rEpochs:   2%|▏         | 11/500 [00:25<18:36,  2.28s/it]\rEpochs:   2%|▏         | 12/500 [00:27<18:16,  2.25s/it]\rEpochs:   3%|▎         | 13/500 [00:30<18:23,  2.27s/it]\rEpochs:   3%|▎         | 14/500 [00:32<18:47,  2.32s/it]\rEpochs:   3%|▎         | 15/500 [00:35<18:38,  2.31s/it]\rEpochs:   3%|▎         | 16/500 [00:37<18:21,  2.28s/it]\rEpochs:   3%|▎         | 17/500 [00:39<18:18,  2.27s/it]\rEpochs:   4%|▎         | 18/500 [00:41<17:53,  2.23s/it]\rEpochs:   4%|▍         | 19/500 [00:43<17:49,  2.22s/it]\rEpochs:   4%|▍         | 20/500 [00:46<17:44,  2.22s/it]\rEpochs:   4%|▍         | 21/500 [00:48<17:56,  2.25s/it]\rEpochs:   4%|▍         | 22/500 [00:50<17:50,  2.24s/it]\rEpochs:   5%|▍         | 23/500 [00:52<17:52,  2.25s/it]\rEpochs:   5%|▍         | 24/500 [00:55<17:49,  2.25s/it]\rEpochs:   5%|▌         | 25/500 [00:57<17:31,  2.21s/it]\rEpochs:   5%|▌         | 26/500 [00:59<17:37,  2.23s/it]\rEpochs:   5%|▌         | 27/500 [01:01<17:27,  2.21s/it]\rEpochs:   6%|▌         | 28/500 [01:03<17:34,  2.23s/it]\rEpochs:   6%|▌         | 29/500 [01:05<17:03,  2.17s/it]\rEpochs:   6%|▌         | 30/500 [01:08<16:50,  2.15s/it]\rEpochs:   6%|▌         | 31/500 [01:10<16:44,  2.14s/it]\rEpochs:   6%|▋         | 32/500 [01:12<16:38,  2.13s/it]\rEpochs:   7%|▋         | 33/500 [01:15<17:57,  2.31s/it]\rEpochs:   7%|▋         | 34/500 [01:17<17:59,  2.32s/it]\rEpochs:   7%|▋         | 35/500 [01:19<17:22,  2.24s/it]\rEpochs:   7%|▋         | 36/500 [01:21<16:45,  2.17s/it]\rEpochs:   7%|▋         | 37/500 [01:23<16:36,  2.15s/it]\rEpochs:   8%|▊         | 38/500 [01:25<16:18,  2.12s/it]\rEpochs:   8%|▊         | 39/500 [01:27<16:25,  2.14s/it]\rEpochs:   8%|▊         | 40/500 [01:29<16:26,  2.15s/it]\rEpochs:   8%|▊         | 41/500 [01:32<16:33,  2.17s/it]\rEpochs:   8%|▊         | 42/500 [01:34<16:29,  2.16s/it]\rEpochs:   9%|▊         | 43/500 [01:36<16:13,  2.13s/it]\rEpochs:   9%|▉         | 44/500 [01:38<15:52,  2.09s/it]\rEpochs:   9%|▉         | 45/500 [01:40<15:45,  2.08s/it]\rEpochs:   9%|▉         | 46/500 [01:42<15:40,  2.07s/it]\rEpochs:   9%|▉         | 47/500 [01:44<15:40,  2.08s/it]\rEpochs:  10%|▉         | 48/500 [01:46<15:38,  2.08s/it]\rEpochs:  10%|▉         | 49/500 [01:48<15:58,  2.13s/it]\rEpochs:  10%|█         | 50/500 [01:50<15:57,  2.13s/it]\rEpochs:  10%|█         | 51/500 [01:53<15:56,  2.13s/it]\rEpochs:  10%|█         | 52/500 [01:55<16:01,  2.15s/it]\rEpochs:  11%|█         | 53/500 [01:57<15:34,  2.09s/it]\rEpochs:  11%|█         | 54/500 [01:59<15:36,  2.10s/it]\rEpochs:  11%|█         | 55/500 [02:01<15:33,  2.10s/it]\rEpochs:  11%|█         | 56/500 [02:03<16:10,  2.19s/it]\rEpochs:  11%|█▏        | 57/500 [02:05<15:50,  2.15s/it]\rEpochs:  12%|█▏        | 58/500 [02:08<15:45,  2.14s/it]\rEpochs:  12%|█▏        | 59/500 [02:10<15:48,  2.15s/it]\rEpochs:  12%|█▏        | 60/500 [02:12<16:19,  2.23s/it]\rEpochs:  12%|█▏        | 61/500 [02:15<17:22,  2.38s/it]\rEpochs:  12%|█▏        | 62/500 [02:17<17:20,  2.38s/it]\rEpochs:  13%|█▎        | 63/500 [02:19<16:37,  2.28s/it]\rEpochs:  13%|█▎        | 64/500 [02:21<16:06,  2.22s/it]\rEpochs:  13%|█▎        | 65/500 [02:23<15:45,  2.17s/it]\rEpochs:  13%|█▎        | 66/500 [02:25<15:25,  2.13s/it]\rEpochs:  13%|█▎        | 67/500 [02:28<15:27,  2.14s/it]\rEpochs:  14%|█▎        | 68/500 [02:30<15:24,  2.14s/it]\rEpochs:  14%|█▍        | 69/500 [02:32<15:15,  2.12s/it]\rEpochs:  14%|█▍        | 70/500 [02:34<15:10,  2.12s/it]\rEpochs:  14%|█▍        | 71/500 [02:36<15:06,  2.11s/it]\rEpochs:  14%|█▍        | 72/500 [02:38<14:47,  2.07s/it]\rEpochs:  15%|█▍        | 73/500 [02:40<14:51,  2.09s/it]\rEpochs:  15%|█▍        | 74/500 [02:42<14:45,  2.08s/it]\rEpochs:  15%|█▌        | 75/500 [02:44<14:46,  2.09s/it]\rEpochs:  15%|█▌        | 76/500 [02:46<14:43,  2.08s/it]\rEpochs:  15%|█▌        | 77/500 [02:49<14:57,  2.12s/it]\rEpochs:  16%|█▌        | 78/500 [02:51<14:50,  2.11s/it]\rEpochs:  16%|█▌        | 79/500 [02:53<14:59,  2.14s/it]\rEpochs:  16%|█▌        | 80/500 [02:55<14:56,  2.13s/it]\rEpochs:  16%|█▌        | 81/500 [02:57<14:45,  2.11s/it]\rEpochs:  16%|█▋        | 82/500 [02:59<14:47,  2.12s/it]\rEpochs:  17%|█▋        | 83/500 [03:01<14:40,  2.11s/it]\rEpochs:  17%|█▋        | 84/500 [03:04<15:22,  2.22s/it]\rEpochs:  17%|█▋        | 85/500 [03:06<14:58,  2.16s/it]\rEpochs:  17%|█▋        | 86/500 [03:08<15:02,  2.18s/it]\rEpochs:  17%|█▋        | 87/500 [03:10<14:56,  2.17s/it]\rEpochs:  18%|█▊        | 88/500 [03:12<15:11,  2.21s/it]\rEpochs:  18%|█▊        | 89/500 [03:15<16:09,  2.36s/it]\rEpochs:  18%|█▊        | 90/500 [03:17<15:43,  2.30s/it]\rEpochs:  18%|█▊        | 91/500 [03:19<15:17,  2.24s/it]\rEpochs:  18%|█▊        | 92/500 [03:22<15:04,  2.22s/it]\rEpochs:  19%|█▊        | 93/500 [03:24<14:58,  2.21s/it]\rEpochs:  19%|█▉        | 94/500 [03:26<14:39,  2.17s/it]\rEpochs:  19%|█▉        | 95/500 [03:28<14:45,  2.19s/it]\rEpochs:  19%|█▉        | 96/500 [03:30<14:31,  2.16s/it]\rEpochs:  19%|█▉        | 97/500 [03:32<14:35,  2.17s/it]\rEpochs:  20%|█▉        | 98/500 [03:34<14:20,  2.14s/it]\rEpochs:  20%|█▉        | 99/500 [03:37<14:10,  2.12s/it]\rEpochs:  20%|██        | 100/500 [03:39<14:06,  2.12s/it]\rEpochs:  20%|██        | 101/500 [03:41<14:00,  2.11s/it]\rEpochs:  20%|██        | 102/500 [03:43<14:08,  2.13s/it]\rEpochs:  21%|██        | 103/500 [03:45<14:08,  2.14s/it]\rEpochs:  21%|██        | 104/500 [03:47<14:19,  2.17s/it]\rEpochs:  21%|██        | 105/500 [03:50<14:34,  2.21s/it]\rEpochs:  21%|██        | 106/500 [03:52<14:48,  2.26s/it]\rEpochs:  21%|██▏       | 107/500 [03:54<14:54,  2.28s/it]\rEpochs:  22%|██▏       | 108/500 [03:56<14:33,  2.23s/it]\rEpochs:  22%|██▏       | 109/500 [03:59<14:39,  2.25s/it]\rEpochs:  22%|██▏       | 110/500 [04:01<14:17,  2.20s/it]\rEpochs:  22%|██▏       | 111/500 [04:03<14:17,  2.20s/it]\rEpochs:  22%|██▏       | 112/500 [04:05<14:20,  2.22s/it]\rEpochs:  23%|██▎       | 113/500 [04:07<14:15,  2.21s/it]\rEpochs:  23%|██▎       | 114/500 [04:10<14:19,  2.23s/it]\rEpochs:  23%|██▎       | 115/500 [04:12<14:46,  2.30s/it]\rEpochs:  23%|██▎       | 116/500 [04:15<15:28,  2.42s/it]\rEpochs:  23%|██▎       | 117/500 [04:17<15:25,  2.42s/it]\rEpochs:  24%|██▎       | 118/500 [04:20<14:58,  2.35s/it]\rEpochs:  24%|██▍       | 119/500 [04:22<14:28,  2.28s/it]\rEpochs:  24%|██▍       | 120/500 [04:24<14:06,  2.23s/it]\rEpochs:  24%|██▍       | 121/500 [04:26<13:46,  2.18s/it]\rEpochs:  24%|██▍       | 122/500 [04:28<13:45,  2.18s/it]\rEpochs:  25%|██▍       | 123/500 [04:30<13:28,  2.15s/it]\rEpochs:  25%|██▍       | 124/500 [04:32<13:33,  2.16s/it]\rEpochs:  25%|██▌       | 125/500 [04:34<13:17,  2.13s/it]\rEpochs:  25%|██▌       | 126/500 [04:36<13:13,  2.12s/it]\rEpochs:  25%|██▌       | 127/500 [04:38<13:06,  2.11s/it]\rEpochs:  26%|██▌       | 128/500 [04:41<13:32,  2.18s/it]\rEpochs:  26%|██▌       | 129/500 [04:43<13:44,  2.22s/it]\rEpochs:  26%|██▌       | 130/500 [04:45<13:37,  2.21s/it]\rEpochs:  26%|██▌       | 131/500 [04:48<13:48,  2.24s/it]\rEpochs:  26%|██▋       | 132/500 [04:50<13:54,  2.27s/it]\rEpochs:  27%|██▋       | 133/500 [04:52<13:58,  2.28s/it]\rEpochs:  27%|██▋       | 134/500 [04:55<14:03,  2.31s/it]\rEpochs:  27%|██▋       | 135/500 [04:57<14:00,  2.30s/it]\rEpochs:  27%|██▋       | 136/500 [04:59<14:01,  2.31s/it]\rEpochs:  27%|██▋       | 137/500 [05:01<13:43,  2.27s/it]\rEpochs:  28%|██▊       | 138/500 [05:04<13:32,  2.25s/it]\rEpochs:  28%|██▊       | 139/500 [05:06<13:21,  2.22s/it]\rEpochs:  28%|██▊       | 140/500 [05:08<13:46,  2.30s/it]\rEpochs:  28%|██▊       | 141/500 [05:10<13:36,  2.27s/it]\rEpochs:  28%|██▊       | 142/500 [05:13<13:57,  2.34s/it]\rEpochs:  29%|██▊       | 143/500 [05:16<14:39,  2.46s/it]\rEpochs:  29%|██▉       | 144/500 [05:18<15:06,  2.55s/it]\rEpochs:  29%|██▉       | 145/500 [05:21<14:51,  2.51s/it]\rEpochs:  29%|██▉       | 146/500 [05:23<14:28,  2.45s/it]\rEpochs:  29%|██▉       | 147/500 [05:25<14:04,  2.39s/it]\rEpochs:  30%|██▉       | 148/500 [05:28<14:05,  2.40s/it]\rEpochs:  30%|██▉       | 149/500 [05:30<13:41,  2.34s/it]\rEpochs:  30%|███       | 150/500 [05:32<13:35,  2.33s/it]\rEpochs:  30%|███       | 151/500 [05:35<13:10,  2.27s/it]\rEpochs:  30%|███       | 152/500 [05:37<12:57,  2.23s/it]\rEpochs:  31%|███       | 153/500 [05:39<12:41,  2.19s/it]\rEpochs:  31%|███       | 154/500 [05:41<12:17,  2.13s/it]\rEpochs:  31%|███       | 155/500 [05:43<12:15,  2.13s/it]\rEpochs:  31%|███       | 156/500 [05:45<12:06,  2.11s/it]\rEpochs:  31%|███▏      | 157/500 [05:47<12:15,  2.14s/it]\rEpochs:  32%|███▏      | 158/500 [05:49<12:14,  2.15s/it]\rEpochs:  32%|███▏      | 159/500 [05:51<12:13,  2.15s/it]\rEpochs:  32%|███▏      | 160/500 [05:54<12:17,  2.17s/it]\rEpochs:  32%|███▏      | 161/500 [05:56<12:18,  2.18s/it]\rEpochs:  32%|███▏      | 162/500 [05:58<12:01,  2.13s/it]\rEpochs:  33%|███▎      | 163/500 [06:00<11:56,  2.13s/it]\rEpochs:  33%|███▎      | 164/500 [06:02<11:45,  2.10s/it]\rEpochs:  33%|███▎      | 165/500 [06:04<11:49,  2.12s/it]\rEpochs:  33%|███▎      | 166/500 [06:07<12:12,  2.19s/it]\rEpochs:  33%|███▎      | 167/500 [06:09<12:29,  2.25s/it]\rEpochs:  34%|███▎      | 168/500 [06:11<12:28,  2.25s/it]\rEpochs:  34%|███▍      | 169/500 [06:14<12:49,  2.33s/it]\rEpochs:  34%|███▍      | 170/500 [06:16<12:53,  2.34s/it]\rEpochs:  34%|███▍      | 171/500 [06:18<12:35,  2.30s/it]\rEpochs:  34%|███▍      | 172/500 [06:20<12:09,  2.22s/it]\rEpochs:  35%|███▍      | 173/500 [06:22<11:51,  2.18s/it]\rEpochs:  35%|███▍      | 174/500 [06:25<11:57,  2.20s/it]\rEpochs:  35%|███▌      | 175/500 [06:27<11:52,  2.19s/it]\rEpochs:  35%|███▌      | 176/500 [06:29<11:59,  2.22s/it]\rEpochs:  35%|███▌      | 177/500 [06:31<11:49,  2.20s/it]\rEpochs:  36%|███▌      | 178/500 [06:33<11:42,  2.18s/it]\rEpochs:  36%|███▌      | 179/500 [06:36<11:29,  2.15s/it]\rEpochs:  36%|███▌      | 180/500 [06:37<11:11,  2.10s/it]\rEpochs:  36%|███▌      | 181/500 [06:40<11:06,  2.09s/it]\rEpochs:  36%|███▋      | 182/500 [06:42<10:58,  2.07s/it]\rEpochs:  37%|███▋      | 183/500 [06:44<10:56,  2.07s/it]\rEpochs:  37%|███▋      | 184/500 [06:46<10:43,  2.04s/it]\rEpochs:  37%|███▋      | 185/500 [06:48<10:53,  2.08s/it]\rEpochs:  37%|███▋      | 186/500 [06:50<10:55,  2.09s/it]\rEpochs:  37%|███▋      | 187/500 [06:52<11:04,  2.12s/it]\rEpochs:  38%|███▊      | 188/500 [06:54<11:07,  2.14s/it]\rEpochs:  38%|███▊      | 189/500 [06:56<10:58,  2.12s/it]\rEpochs:  38%|███▊      | 190/500 [06:58<10:56,  2.12s/it]\rEpochs:  38%|███▊      | 191/500 [07:01<10:50,  2.11s/it]\rEpochs:  38%|███▊      | 192/500 [07:03<10:48,  2.11s/it]\rEpochs:  39%|███▊      | 193/500 [07:05<10:52,  2.13s/it]\rEpochs:  39%|███▉      | 194/500 [07:07<10:52,  2.13s/it]\rEpochs:  39%|███▉      | 195/500 [07:09<10:48,  2.13s/it]\rEpochs:  39%|███▉      | 196/500 [07:11<10:51,  2.14s/it]\rEpochs:  39%|███▉      | 197/500 [07:14<11:19,  2.24s/it]\rEpochs:  40%|███▉      | 198/500 [07:16<11:33,  2.30s/it]\rEpochs:  40%|███▉      | 199/500 [07:18<11:26,  2.28s/it]\rEpochs:  40%|████      | 200/500 [07:20<11:03,  2.21s/it]\rEpochs:  40%|████      | 201/500 [07:23<11:05,  2.22s/it]\rEpochs:  40%|████      | 202/500 [07:25<10:58,  2.21s/it]\rEpochs:  41%|████      | 203/500 [07:27<10:47,  2.18s/it]\rEpochs:  41%|████      | 204/500 [07:29<10:42,  2.17s/it]\rEpochs:  41%|████      | 205/500 [07:31<10:34,  2.15s/it]\rEpochs:  41%|████      | 206/500 [07:33<10:35,  2.16s/it]\rEpochs:  41%|████▏     | 207/500 [07:36<10:31,  2.16s/it]\rEpochs:  42%|████▏     | 208/500 [07:38<10:20,  2.13s/it]\rEpochs:  42%|████▏     | 209/500 [07:40<10:08,  2.09s/it]\rEpochs:  42%|████▏     | 210/500 [07:42<10:01,  2.08s/it]\rEpochs:  42%|████▏     | 211/500 [07:44<10:04,  2.09s/it]\rEpochs:  42%|████▏     | 212/500 [07:46<10:01,  2.09s/it]\rEpochs:  43%|████▎     | 213/500 [07:48<10:22,  2.17s/it]\rEpochs:  43%|████▎     | 214/500 [07:51<10:26,  2.19s/it]\rEpochs:  43%|████▎     | 215/500 [07:53<10:37,  2.24s/it]\rEpochs:  43%|████▎     | 216/500 [07:55<10:34,  2.24s/it]\rEpochs:  43%|████▎     | 217/500 [07:57<10:17,  2.18s/it]\rEpochs:  44%|████▎     | 218/500 [07:59<10:12,  2.17s/it]\rEpochs:  44%|████▍     | 219/500 [08:01<10:01,  2.14s/it]\rEpochs:  44%|████▍     | 220/500 [08:03<09:53,  2.12s/it]\rEpochs:  44%|████▍     | 221/500 [08:06<09:52,  2.12s/it]\rEpochs:  44%|████▍     | 222/500 [08:08<10:01,  2.16s/it]\rEpochs:  45%|████▍     | 223/500 [08:10<09:56,  2.15s/it]\rEpochs:  45%|████▍     | 224/500 [08:12<10:15,  2.23s/it]\rEpochs:  45%|████▌     | 225/500 [08:15<10:52,  2.37s/it]\rEpochs:  45%|████▌     | 226/500 [08:17<10:33,  2.31s/it]\rEpochs:  45%|████▌     | 227/500 [08:19<10:15,  2.25s/it]\rEpochs:  46%|████▌     | 228/500 [08:21<09:58,  2.20s/it]\rEpochs:  46%|████▌     | 229/500 [08:24<09:47,  2.17s/it]\rEpochs:  46%|████▌     | 230/500 [08:26<09:46,  2.17s/it]\rEpochs:  46%|████▌     | 231/500 [08:28<09:39,  2.15s/it]\rEpochs:  46%|████▋     | 232/500 [08:30<09:30,  2.13s/it]\rEpochs:  47%|████▋     | 233/500 [08:32<09:35,  2.16s/it]\rEpochs:  47%|████▋     | 234/500 [08:34<09:19,  2.10s/it]\rEpochs:  47%|████▋     | 235/500 [08:36<09:13,  2.09s/it]\rEpochs:  47%|████▋     | 236/500 [08:38<09:06,  2.07s/it]\rEpochs:  47%|████▋     | 237/500 [08:40<09:07,  2.08s/it]\rEpochs:  48%|████▊     | 238/500 [08:42<09:05,  2.08s/it]\rEpochs:  48%|████▊     | 239/500 [08:44<09:01,  2.08s/it]\rEpochs:  48%|████▊     | 240/500 [08:46<08:56,  2.06s/it]\rEpochs:  48%|████▊     | 241/500 [08:49<08:59,  2.08s/it]\rEpochs:  48%|████▊     | 242/500 [08:51<08:57,  2.08s/it]\rEpochs:  49%|████▊     | 243/500 [08:53<09:02,  2.11s/it]\rEpochs:  49%|████▉     | 244/500 [08:55<09:01,  2.12s/it]\rEpochs:  49%|████▉     | 245/500 [08:57<08:51,  2.08s/it]\rEpochs:  49%|████▉     | 246/500 [08:59<08:50,  2.09s/it]\rEpochs:  49%|████▉     | 247/500 [09:01<08:39,  2.05s/it]\rEpochs:  50%|████▉     | 248/500 [09:03<08:32,  2.03s/it]\rEpochs:  50%|████▉     | 249/500 [09:05<08:30,  2.03s/it]\rEpochs:  50%|█████     | 250/500 [09:07<08:22,  2.01s/it]\rEpochs:  50%|█████     | 251/500 [09:09<08:31,  2.06s/it]\rEpochs:  50%|█████     | 252/500 [09:11<08:27,  2.04s/it]\rEpochs:  51%|█████     | 253/500 [09:14<08:55,  2.17s/it]\rEpochs:  51%|█████     | 254/500 [09:16<09:03,  2.21s/it]\rEpochs:  51%|█████     | 255/500 [09:18<08:47,  2.15s/it]\rEpochs:  51%|█████     | 256/500 [09:20<08:42,  2.14s/it]\rEpochs:  51%|█████▏    | 257/500 [09:22<08:34,  2.12s/it]\rEpochs:  52%|█████▏    | 258/500 [09:24<08:26,  2.09s/it]\rEpochs:  52%|█████▏    | 259/500 [09:26<08:20,  2.08s/it]\rEpochs:  52%|█████▏    | 260/500 [09:28<08:25,  2.11s/it]\rEpochs:  52%|█████▏    | 261/500 [09:30<08:14,  2.07s/it]\rEpochs:  52%|█████▏    | 262/500 [09:33<08:23,  2.11s/it]\rEpochs:  53%|█████▎    | 263/500 [09:35<08:39,  2.19s/it]\rEpochs:  53%|█████▎    | 264/500 [09:37<08:33,  2.18s/it]\rEpochs:  53%|█████▎    | 265/500 [09:39<08:26,  2.16s/it]\rEpochs:  53%|█████▎    | 266/500 [09:41<08:23,  2.15s/it]\rEpochs:  53%|█████▎    | 267/500 [09:44<08:20,  2.15s/it]\rEpochs:  54%|█████▎    | 268/500 [09:46<08:20,  2.16s/it]\rEpochs:  54%|█████▍    | 269/500 [09:48<08:20,  2.17s/it]\rEpochs:  54%|█████▍    | 270/500 [09:50<08:13,  2.14s/it]\rEpochs:  54%|█████▍    | 271/500 [09:52<08:13,  2.16s/it]\rEpochs:  54%|█████▍    | 272/500 [09:54<08:06,  2.13s/it]\rEpochs:  55%|█████▍    | 273/500 [09:56<08:07,  2.15s/it]\rEpochs:  55%|█████▍    | 274/500 [09:59<08:11,  2.17s/it]\rEpochs:  55%|█████▌    | 275/500 [10:01<08:05,  2.16s/it]\rEpochs:  55%|█████▌    | 276/500 [10:03<08:00,  2.15s/it]\rEpochs:  55%|█████▌    | 277/500 [10:05<08:01,  2.16s/it]\rEpochs:  56%|█████▌    | 278/500 [10:07<08:05,  2.19s/it]\rEpochs:  56%|█████▌    | 279/500 [10:10<08:11,  2.22s/it]\rEpochs:  56%|█████▌    | 280/500 [10:12<08:19,  2.27s/it]\rEpochs:  56%|█████▌    | 281/500 [10:15<08:49,  2.42s/it]\rEpochs:  56%|█████▋    | 282/500 [10:17<08:58,  2.47s/it]\rEpochs:  57%|█████▋    | 283/500 [10:20<08:54,  2.46s/it]\rEpochs:  57%|█████▋    | 284/500 [10:22<08:27,  2.35s/it]\rEpochs:  57%|█████▋    | 285/500 [10:24<08:06,  2.26s/it]\rEpochs:  57%|█████▋    | 286/500 [10:26<07:50,  2.20s/it]\rEpochs:  57%|█████▋    | 287/500 [10:28<07:44,  2.18s/it]\rEpochs:  58%|█████▊    | 288/500 [10:30<07:39,  2.17s/it]\rEpochs:  58%|█████▊    | 289/500 [10:32<07:32,  2.15s/it]\rEpochs:  58%|█████▊    | 290/500 [10:34<07:25,  2.12s/it]\rEpochs:  58%|█████▊    | 291/500 [10:37<07:21,  2.11s/it]\rEpochs:  58%|█████▊    | 292/500 [10:39<07:08,  2.06s/it]\rEpochs:  59%|█████▊    | 293/500 [10:41<07:06,  2.06s/it]\rEpochs:  59%|█████▉    | 294/500 [10:43<07:06,  2.07s/it]\rEpochs:  59%|█████▉    | 295/500 [10:45<07:10,  2.10s/it]\rEpochs:  59%|█████▉    | 296/500 [10:47<07:12,  2.12s/it]\rEpochs:  59%|█████▉    | 297/500 [10:49<07:14,  2.14s/it]\rEpochs:  60%|█████▉    | 298/500 [10:51<07:11,  2.14s/it]\rEpochs:  60%|█████▉    | 299/500 [10:53<07:08,  2.13s/it]\rEpochs:  60%|██████    | 300/500 [10:55<07:01,  2.11s/it]\rEpochs:  60%|██████    | 301/500 [10:58<07:00,  2.11s/it]\rEpochs:  60%|██████    | 302/500 [11:00<07:01,  2.13s/it]\rEpochs:  61%|██████    | 303/500 [11:02<06:54,  2.10s/it]\rEpochs:  61%|██████    | 304/500 [11:04<06:44,  2.06s/it]\rEpochs:  61%|██████    | 305/500 [11:06<06:40,  2.05s/it]\rEpochs:  61%|██████    | 306/500 [11:08<06:42,  2.08s/it]\rEpochs:  61%|██████▏   | 307/500 [11:10<06:56,  2.16s/it]\rEpochs:  62%|██████▏   | 308/500 [11:12<06:53,  2.15s/it]\rEpochs:  62%|██████▏   | 309/500 [11:15<07:17,  2.29s/it]\rEpochs:  62%|██████▏   | 310/500 [11:17<07:04,  2.23s/it]\rEpochs:  62%|██████▏   | 311/500 [11:19<06:50,  2.17s/it]\rEpochs:  62%|██████▏   | 312/500 [11:21<06:37,  2.12s/it]\rEpochs:  63%|██████▎   | 313/500 [11:23<06:42,  2.15s/it]\rEpochs:  63%|██████▎   | 314/500 [11:25<06:33,  2.12s/it]\rEpochs:  63%|██████▎   | 315/500 [11:28<06:50,  2.22s/it]\rEpochs:  63%|██████▎   | 316/500 [11:30<06:48,  2.22s/it]\rEpochs:  63%|██████▎   | 317/500 [11:32<06:46,  2.22s/it]\rEpochs:  64%|██████▎   | 318/500 [11:34<06:38,  2.19s/it]\rEpochs:  64%|██████▍   | 319/500 [11:37<06:36,  2.19s/it]\rEpochs:  64%|██████▍   | 320/500 [11:39<06:29,  2.16s/it]\rEpochs:  64%|██████▍   | 321/500 [11:41<06:32,  2.19s/it]\rEpochs:  64%|██████▍   | 322/500 [11:43<06:28,  2.18s/it]\rEpochs:  65%|██████▍   | 323/500 [11:45<06:23,  2.17s/it]\rEpochs:  65%|██████▍   | 324/500 [11:48<06:25,  2.19s/it]\rEpochs:  65%|██████▌   | 325/500 [11:50<06:25,  2.20s/it]\rEpochs:  65%|██████▌   | 326/500 [11:52<06:19,  2.18s/it]\rEpochs:  65%|██████▌   | 327/500 [11:54<06:16,  2.18s/it]\rEpochs:  66%|██████▌   | 328/500 [11:56<06:10,  2.15s/it]\rEpochs:  66%|██████▌   | 329/500 [11:58<06:11,  2.17s/it]\rEpochs:  66%|██████▌   | 330/500 [12:00<06:04,  2.14s/it]\rEpochs:  66%|██████▌   | 331/500 [12:03<05:58,  2.12s/it]\rEpochs:  66%|██████▋   | 332/500 [12:05<05:50,  2.09s/it]\rEpochs:  67%|██████▋   | 333/500 [12:07<05:45,  2.07s/it]\rEpochs:  67%|██████▋   | 334/500 [12:09<05:51,  2.12s/it]\rEpochs:  67%|██████▋   | 335/500 [12:11<05:47,  2.10s/it]\rEpochs:  67%|██████▋   | 336/500 [12:13<05:59,  2.19s/it]\rEpochs:  67%|██████▋   | 337/500 [12:16<06:13,  2.29s/it]\rEpochs:  68%|██████▊   | 338/500 [12:18<06:02,  2.24s/it]\rEpochs:  68%|██████▊   | 339/500 [12:20<05:46,  2.15s/it]\rEpochs:  68%|██████▊   | 340/500 [12:22<05:36,  2.11s/it]\rEpochs:  68%|██████▊   | 341/500 [12:24<05:35,  2.11s/it]\rEpochs:  68%|██████▊   | 342/500 [12:26<05:29,  2.09s/it]\rEpochs:  69%|██████▊   | 343/500 [12:28<05:31,  2.11s/it]\rEpochs:  69%|██████▉   | 344/500 [12:30<05:18,  2.04s/it]\rEpochs:  69%|██████▉   | 345/500 [12:32<05:21,  2.07s/it]\rEpochs:  69%|██████▉   | 346/500 [12:34<05:17,  2.06s/it]\rEpochs:  69%|██████▉   | 347/500 [12:36<05:18,  2.08s/it]\rEpochs:  70%|██████▉   | 348/500 [12:38<05:15,  2.08s/it]\rEpochs:  70%|██████▉   | 349/500 [12:41<05:24,  2.15s/it]\rEpochs:  70%|███████   | 350/500 [12:43<05:22,  2.15s/it]\rEpochs:  70%|███████   | 351/500 [12:45<05:16,  2.12s/it]\rEpochs:  70%|███████   | 352/500 [12:47<05:16,  2.14s/it]\rEpochs:  71%|███████   | 353/500 [12:49<05:17,  2.16s/it]\rEpochs:  71%|███████   | 354/500 [12:52<05:17,  2.17s/it]\rEpochs:  71%|███████   | 355/500 [12:54<05:16,  2.18s/it]\rEpochs:  71%|███████   | 356/500 [12:56<05:14,  2.19s/it]\rEpochs:  71%|███████▏  | 357/500 [12:58<05:15,  2.20s/it]\rEpochs:  72%|███████▏  | 358/500 [13:00<05:03,  2.14s/it]\rEpochs:  72%|███████▏  | 359/500 [13:02<05:00,  2.13s/it]\rEpochs:  72%|███████▏  | 360/500 [13:04<04:57,  2.12s/it]\rEpochs:  72%|███████▏  | 361/500 [13:06<04:48,  2.07s/it]\rEpochs:  72%|███████▏  | 362/500 [13:09<04:49,  2.10s/it]\rEpochs:  73%|███████▎  | 363/500 [13:11<04:59,  2.18s/it]\rEpochs:  73%|███████▎  | 364/500 [13:13<05:02,  2.23s/it]\rEpochs:  73%|███████▎  | 365/500 [13:16<05:10,  2.30s/it]\rEpochs:  73%|███████▎  | 366/500 [13:18<05:03,  2.26s/it]\rEpochs:  73%|███████▎  | 367/500 [13:20<04:52,  2.20s/it]\rEpochs:  74%|███████▎  | 368/500 [13:22<04:44,  2.15s/it]\rEpochs:  74%|███████▍  | 369/500 [13:24<04:47,  2.19s/it]\rEpochs:  74%|███████▍  | 370/500 [13:26<04:39,  2.15s/it]\rEpochs:  74%|███████▍  | 371/500 [13:29<04:39,  2.17s/it]\rEpochs:  74%|███████▍  | 372/500 [13:31<04:34,  2.15s/it]\rEpochs:  75%|███████▍  | 373/500 [13:33<04:35,  2.17s/it]\rEpochs:  75%|███████▍  | 374/500 [13:35<04:30,  2.15s/it]\rEpochs:  75%|███████▌  | 375/500 [13:37<04:28,  2.15s/it]\rEpochs:  75%|███████▌  | 376/500 [13:39<04:21,  2.11s/it]\rEpochs:  75%|███████▌  | 377/500 [13:41<04:15,  2.08s/it]\rEpochs:  76%|███████▌  | 378/500 [13:43<04:15,  2.09s/it]\rEpochs:  76%|███████▌  | 379/500 [13:45<04:12,  2.09s/it]\rEpochs:  76%|███████▌  | 380/500 [13:48<04:17,  2.15s/it]\rEpochs:  76%|███████▌  | 381/500 [13:50<04:12,  2.13s/it]\rEpochs:  76%|███████▋  | 382/500 [13:52<04:08,  2.10s/it]\rEpochs:  77%|███████▋  | 383/500 [13:54<04:08,  2.12s/it]\rEpochs:  77%|███████▋  | 384/500 [13:56<04:04,  2.11s/it]\rEpochs:  77%|███████▋  | 385/500 [13:58<04:02,  2.10s/it]\rEpochs:  77%|███████▋  | 386/500 [14:00<03:57,  2.09s/it]\rEpochs:  77%|███████▋  | 387/500 [14:02<03:53,  2.07s/it]\rEpochs:  78%|███████▊  | 388/500 [14:04<03:50,  2.06s/it]\rEpochs:  78%|███████▊  | 389/500 [14:06<03:48,  2.06s/it]\rEpochs:  78%|███████▊  | 390/500 [14:08<03:46,  2.06s/it]\rEpochs:  78%|███████▊  | 391/500 [14:11<03:50,  2.11s/it]\rEpochs:  78%|███████▊  | 392/500 [14:13<03:59,  2.21s/it]\rEpochs:  79%|███████▊  | 393/500 [14:16<04:10,  2.34s/it]\rEpochs:  79%|███████▉  | 394/500 [14:18<04:05,  2.31s/it]\rEpochs:  79%|███████▉  | 395/500 [14:20<03:58,  2.28s/it]\rEpochs:  79%|███████▉  | 396/500 [14:22<03:49,  2.20s/it]\rEpochs:  79%|███████▉  | 397/500 [14:24<03:48,  2.22s/it]\rEpochs:  80%|███████▉  | 398/500 [14:26<03:41,  2.17s/it]\rEpochs:  80%|███████▉  | 399/500 [14:29<03:37,  2.16s/it]\rEpochs:  80%|████████  | 400/500 [14:31<03:31,  2.11s/it]\rEpochs:  80%|████████  | 401/500 [14:33<03:30,  2.12s/it]\rEpochs:  80%|████████  | 402/500 [14:35<03:24,  2.09s/it]\rEpochs:  81%|████████  | 403/500 [14:37<03:24,  2.11s/it]\rEpochs:  81%|████████  | 404/500 [14:39<03:24,  2.13s/it]\rEpochs:  81%|████████  | 405/500 [14:41<03:21,  2.12s/it]\rEpochs:  81%|████████  | 406/500 [14:43<03:22,  2.15s/it]\rEpochs:  81%|████████▏ | 407/500 [14:46<03:21,  2.16s/it]\rEpochs:  82%|████████▏ | 408/500 [14:48<03:20,  2.17s/it]\rEpochs:  82%|████████▏ | 409/500 [14:50<03:18,  2.18s/it]\rEpochs:  82%|████████▏ | 410/500 [14:52<03:14,  2.16s/it]\rEpochs:  82%|████████▏ | 411/500 [14:54<03:11,  2.15s/it]\rEpochs:  82%|████████▏ | 412/500 [14:56<03:06,  2.12s/it]\rEpochs:  83%|████████▎ | 413/500 [14:58<03:06,  2.14s/it]\rEpochs:  83%|████████▎ | 414/500 [15:00<02:59,  2.09s/it]\rEpochs:  83%|████████▎ | 415/500 [15:02<02:57,  2.09s/it]\rEpochs:  83%|████████▎ | 416/500 [15:04<02:52,  2.05s/it]\rEpochs:  83%|████████▎ | 417/500 [15:06<02:49,  2.04s/it]\rEpochs:  84%|████████▎ | 418/500 [15:08<02:46,  2.03s/it]\rEpochs:  84%|████████▍ | 419/500 [15:11<02:46,  2.06s/it]\rEpochs:  84%|████████▍ | 420/500 [15:13<02:54,  2.18s/it]\rEpochs:  84%|████████▍ | 421/500 [15:16<03:07,  2.37s/it]\rEpochs:  84%|████████▍ | 422/500 [15:19<03:19,  2.56s/it]\rEpochs:  85%|████████▍ | 423/500 [15:21<03:13,  2.51s/it]\rEpochs:  85%|████████▍ | 424/500 [15:23<03:04,  2.43s/it]\rEpochs:  85%|████████▌ | 425/500 [15:26<02:54,  2.33s/it]\rEpochs:  85%|████████▌ | 426/500 [15:28<02:52,  2.33s/it]\rEpochs:  85%|████████▌ | 427/500 [15:30<02:46,  2.29s/it]\rEpochs:  86%|████████▌ | 428/500 [15:32<02:42,  2.26s/it]\rEpochs:  86%|████████▌ | 429/500 [15:34<02:35,  2.20s/it]\rEpochs:  86%|████████▌ | 430/500 [15:36<02:32,  2.18s/it]\rEpochs:  86%|████████▌ | 431/500 [15:39<02:29,  2.17s/it]\rEpochs:  86%|████████▋ | 432/500 [15:41<02:23,  2.11s/it]\rEpochs:  87%|████████▋ | 433/500 [15:43<02:21,  2.12s/it]\rEpochs:  87%|████████▋ | 434/500 [15:45<02:20,  2.12s/it]\rEpochs:  87%|████████▋ | 435/500 [15:47<02:18,  2.12s/it]\rEpochs:  87%|████████▋ | 436/500 [15:49<02:19,  2.18s/it]\rEpochs:  87%|████████▋ | 437/500 [15:52<02:18,  2.20s/it]\rEpochs:  88%|████████▊ | 438/500 [15:54<02:15,  2.18s/it]\rEpochs:  88%|████████▊ | 439/500 [15:56<02:11,  2.16s/it]\rEpochs:  88%|████████▊ | 440/500 [15:58<02:07,  2.13s/it]\rEpochs:  88%|████████▊ | 441/500 [16:00<02:03,  2.10s/it]\rEpochs:  88%|████████▊ | 442/500 [16:02<02:00,  2.08s/it]\rEpochs:  89%|████████▊ | 443/500 [16:04<01:57,  2.06s/it]\rEpochs:  89%|████████▉ | 444/500 [16:06<01:54,  2.04s/it]\rEpochs:  89%|████████▉ | 445/500 [16:08<01:53,  2.06s/it]\rEpochs:  89%|████████▉ | 446/500 [16:10<01:52,  2.09s/it]\rEpochs:  89%|████████▉ | 447/500 [16:13<01:59,  2.26s/it]\rEpochs:  90%|████████▉ | 448/500 [16:15<02:01,  2.34s/it]\rEpochs:  90%|████████▉ | 449/500 [16:18<01:56,  2.29s/it]\rEpochs:  90%|█████████ | 450/500 [16:20<01:50,  2.21s/it]\rEpochs:  90%|█████████ | 451/500 [16:22<01:45,  2.16s/it]\rEpochs:  90%|█████████ | 452/500 [16:24<01:42,  2.13s/it]\rEpochs:  91%|█████████ | 453/500 [16:26<01:39,  2.11s/it]\rEpochs:  91%|█████████ | 454/500 [16:28<01:36,  2.11s/it]\rEpochs:  91%|█████████ | 455/500 [16:30<01:35,  2.11s/it]\rEpochs:  91%|█████████ | 456/500 [16:32<01:32,  2.11s/it]\rEpochs:  91%|█████████▏| 457/500 [16:34<01:29,  2.08s/it]\rEpochs:  92%|█████████▏| 458/500 [16:36<01:28,  2.10s/it]\rEpochs:  92%|█████████▏| 459/500 [16:38<01:25,  2.09s/it]\rEpochs:  92%|█████████▏| 460/500 [16:40<01:24,  2.10s/it]\rEpochs:  92%|█████████▏| 461/500 [16:43<01:22,  2.11s/it]\rEpochs:  92%|█████████▏| 462/500 [16:45<01:19,  2.09s/it]\rEpochs:  93%|█████████▎| 463/500 [16:47<01:18,  2.11s/it]\rEpochs:  93%|█████████▎| 464/500 [16:49<01:16,  2.13s/it]\rEpochs:  93%|█████████▎| 465/500 [16:51<01:14,  2.14s/it]\rEpochs:  93%|█████████▎| 466/500 [16:53<01:14,  2.18s/it]\rEpochs:  93%|█████████▎| 467/500 [16:56<01:12,  2.19s/it]\rEpochs:  94%|█████████▎| 468/500 [16:58<01:10,  2.19s/it]\rEpochs:  94%|█████████▍| 469/500 [17:00<01:06,  2.16s/it]\rEpochs:  94%|█████████▍| 470/500 [17:02<01:03,  2.12s/it]\rEpochs:  94%|█████████▍| 471/500 [17:04<01:01,  2.12s/it]\rEpochs:  94%|█████████▍| 472/500 [17:06<00:59,  2.11s/it]\rEpochs:  95%|█████████▍| 473/500 [17:08<00:57,  2.12s/it]\rEpochs:  95%|█████████▍| 474/500 [17:10<00:54,  2.09s/it]\rEpochs:  95%|█████████▌| 475/500 [17:13<00:54,  2.20s/it]\rEpochs:  95%|█████████▌| 476/500 [17:15<00:55,  2.33s/it]\rEpochs:  95%|█████████▌| 477/500 [17:18<00:52,  2.29s/it]\rEpochs:  96%|█████████▌| 478/500 [17:20<00:48,  2.22s/it]\rEpochs:  96%|█████████▌| 479/500 [17:22<00:45,  2.16s/it]\rEpochs:  96%|█████████▌| 480/500 [17:24<00:43,  2.16s/it]\rEpochs:  96%|█████████▌| 481/500 [17:26<00:40,  2.12s/it]\rEpochs:  96%|█████████▋| 482/500 [17:28<00:39,  2.17s/it]\rEpochs:  97%|█████████▋| 483/500 [17:30<00:36,  2.17s/it]\rEpochs:  97%|█████████▋| 484/500 [17:32<00:34,  2.16s/it]\rEpochs:  97%|█████████▋| 485/500 [17:34<00:31,  2.11s/it]\rEpochs:  97%|█████████▋| 486/500 [17:37<00:29,  2.12s/it]\rEpochs:  97%|█████████▋| 487/500 [17:39<00:27,  2.09s/it]\rEpochs:  98%|█████████▊| 488/500 [17:41<00:24,  2.06s/it]\rEpochs:  98%|█████████▊| 489/500 [17:43<00:22,  2.06s/it]\rEpochs:  98%|█████████▊| 490/500 [17:45<00:20,  2.06s/it]\rEpochs:  98%|█████████▊| 491/500 [17:47<00:18,  2.10s/it]\rEpochs:  98%|█████████▊| 492/500 [17:49<00:17,  2.14s/it]\rEpochs:  99%|█████████▊| 493/500 [17:51<00:15,  2.15s/it]\rEpochs:  99%|█████████▉| 494/500 [17:53<00:12,  2.17s/it]\rEpochs:  99%|█████████▉| 495/500 [17:56<00:10,  2.14s/it]\rEpochs:  99%|█████████▉| 496/500 [17:58<00:08,  2.12s/it]\rEpochs:  99%|█████████▉| 497/500 [18:00<00:06,  2.11s/it]\rEpochs: 100%|█████████▉| 498/500 [18:02<00:04,  2.09s/it]\rEpochs: 100%|█████████▉| 499/500 [18:04<00:02,  2.09s/it]\rEpochs: 100%|██████████| 500/500 [18:06<00:00,  2.08s/it]\rEpochs: 100%|██████████| 500/500 [18:06<00:00,  2.17s/it]\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "# Train Loop\n",
    "#for params in ParameterGrid(param_grid): # use if experimenting with different hyper parameter configurations\n",
    "for params in [param_grid]:\n",
    "    # Build model with current hyperparameters\n",
    "    model = MLP(input_size=X_train.shape[1],\n",
    "                hidden_layer_sizes=params['hidden_layer_sizes'],\n",
    "                output_size=len(np.unique(y_train)),\n",
    "                activation=params['activation']).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=params['learning_rate_init'], momentum=momentum, weight_decay=params['alpha'])\n",
    "    \n",
    "    # Train the model\n",
    "    train_model(model, optimizer, criterion, train_loader, num_epochs=num_epochs)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy, outputs = evaluate_model(model, criterion, test_loader)\n",
    "    \n",
    "    # Track the best performing model\n",
    "    if test_accuracy > best_score:\n",
    "        best_score = test_accuracy\n",
    "        best_model = model\n",
    "        best_params = params\n",
    "\n",
    "# Evaluate final model\n",
    "train_loss, train_accuracy, _ = evaluate_model(best_model, criterion, train_loader)\n",
    "\n",
    "# Save predictions for confusion matrix\n",
    "test_loss, test_accuracy, predictions = evaluate_model(best_model, criterion, test_loader) \n",
    "\n",
    "results = {\n",
    "    \"params\": best_params,\n",
    "    \"train loss\":  train_loss,\n",
    "    \"train accuracy\": train_accuracy,\n",
    "    \"test loss\": test_loss,\n",
    "    \"test accuracy\": test_accuracy\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f77aae4-4fdd-4f54-b720-852ab37b093c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd16bbda-27c2-4d48-8ec3-8087f796ae9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'learning_rate_init': array(0.001), 'alpha': array(0), 'activation': ['tanh'], 'hidden_layer_sizes': [12011]}\n",
      "Final Train Accuracy: 0.8107916666666667\n",
      "Final Test Accuracy: 0.8201666666666667\n"
     ]
    }
   ],
   "source": [
    "best_params = results[\"params\"]\n",
    "final_train_accuracy = results[\"train accuracy\"]\n",
    "final_test_accuracy = results[\"test accuracy\"]\n",
    "\n",
    "print(f\"Best params: {best_params}\")\n",
    "print(f\"Final Train Accuracy: {train_accuracy*100}%\")\n",
    "print(f\"Final Test Accuracy: {test_accuracy*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e421e659-ea31-45f8-99bf-ca57f562f776",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "Accuracy can be misleading on imbalanced datasets.  A confusion matrix shows the breakdown of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "<img\n",
    "  src ='https://i.ibb.co/dmBHnNV/conf-mat.png'\n",
    "  alt = \"conf_mat.png\"\n",
    "  width=\"300\" \n",
    "  height=\"256\"\n",
    "/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e876caf6-f6cc-40a0-a0a9-dcc0dd5369c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f3a6f3c9bd0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bdb4531-bb06-47eb-bbad-9b4947111243",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "From the above figure, it can be observed that the model has a bias toward false positives.  This makes sense given the dataset ratio between default (22%) and non-default (77%) data points.  As a result, the model might benefit from a more balanced dataset.  However, there are other tweaks that can be made to the training pipeline such as stratified cross-validation which ensures that the class distribution is consistent across training and test sets. This prevents overfitting to the majority class, which can sometimes lead to a high false positive rate."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PyTorch ML Demo",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
